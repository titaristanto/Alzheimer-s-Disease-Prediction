from __future__ import division
import numpy as np
import matplotlib.pyplot as plt
import os, itertools, subprocess
import pandas as pd
from sklearn.tree import DecisionTreeClassifier, export_graphviz
from sklearn import preprocessing
import plotly.plotly as py
import plotly.figure_factory as ff
from sklearn.metrics import confusion_matrix, accuracy_score

def load_data():
    os.chdir('C:\\Users\\E460\\PycharmProjects\\untitled3\\CS229\\project')
    df = pd.read_csv('Final_filtered_data.csv')
    raw_gen=pd.concat([df.iloc[:,1:5],df.iloc[:,6:14]],axis=1,join='inner')
    raw_MRI=df.iloc[:,14:136]
    raw_data=pd.concat([raw_gen,raw_MRI],axis=1,join='inner')
    return raw_data

def preprocess_data(raw_data):
    # Dropping missing values
    raw_data_cleaned=raw_data.dropna()
    raw_data_cleaned=raw_data_cleaned[(raw_data_cleaned.iloc[:,14:136]!=' ').all(1)]

    # Set some features as categorical
    xcat_p = raw_data_cleaned[['PTGENDER','PTMARRY']]
    raw_data_cleaned.drop(['DX_bl','PTGENDER','PTMARRY'], axis=1, inplace=True)
    #PTGENDER: 0:Female; 1: Male -- #PTMARRY: 0:Divorced; 1: Married; 2: Never Married 4:Widowed

    y_p = raw_data_cleaned[['DX']]
    raw_data_cleaned.drop(['DX'], axis=1, inplace=True)
    #DX: 0: Dementia, 1:MCI to Dementia; 2: MCI; 3: NL

    le = preprocessing.LabelEncoder()
    xcat=xcat_p.apply(le.fit_transform)
    x=pd.concat([xcat,raw_data_cleaned],axis=1,join='inner')

    # Set 'DX' (Demented or Not) as categorical
    y=y_p.apply(le.fit_transform)
    return x,y


def split_data(x,y):
    train_split=0.7 # fraction of the data used in the training set
    m=x.shape[0] # number of data points

    x_train=x.iloc[0:int(m*train_split),:]
    y_train=y.iloc[0:int(m*train_split),:]
    x_test=x.iloc[int(m*train_split)+1:m-1,:]
    y_test=y.iloc[int(m*train_split)+1:m-1,:]
    return x_train, y_train, x_test, y_test

def decision_tree(x,y):
    clf=DecisionTreeClassifier(criterion="gini",min_samples_split=20,random_state=0)
    clf.fit(x,y)

    #cross_val_score(clf, x, y, cv=10)
    return clf

def feature_importances(x, clf):
    importances = clf.feature_importances_
    indices = np.argsort(importances)[::-1]

    # Print the feature ranking
    print("Feature ranking:")
    header_sorted=[]
    for f in range(len(list(x))):
        header_sorted.append(list(x)[indices[f]])
        print("%d. Feature: %s (%f)" % (f + 1, list(x)[indices[f]], importances[indices[f]]))

    # Plot the feature importances
    plt.figure()
    plt.title("Feature importances")
    plt.bar(range(x.shape[1]), importances[indices], color="r", align="center")
    plt.xticks(range(x.shape[1]), header_sorted)
    plt.xlim([-1, x.shape[1]])
    plt.savefig('feature_importance_tadpole.png')
    plt.show()

def visualize_tree(tree, feature_names):
    with open("dt.dot", 'w') as f:
        export_graphviz(tree, out_file=f,
                        feature_names=feature_names)

    command = ["dot", "-Tpng", "dt.dot", "-o", "dt.png"]
    try:
        subprocess.check_call(command)
    except:
        exit("Could not run dot, ie graphviz, to "
             "produce visualization")

def scatterplot_matrix(x,y):
    dat=pd.concat([x,y],axis=1,join='inner')
    fig = ff.create_scatterplotmatrix(dat, diag='histogram', index='Group',
                                  height=800, width=800)
    py.iplot(fig, filename='Histograms along Diagonal Subplots')

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix')

    print(cm)

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.savefig('dementia_Tadpole.png')

def main():
    raw_data=load_data()
    x,y=preprocess_data(raw_data)
    x_train, y_train, x_test, y_test=split_data(x,y)
    clf=decision_tree(x_train, y_train)
    y_pred=clf.predict(x_test)

    # Confusion Matrix
    cnf_matrix=confusion_matrix(y_test, y_pred)
    #DX: 0: Dementia, 1:MCI to Dementia; 2: MCI; 3: NL
    class_names=list(['Dementia','MCI','NL','MCI to Dementia'])
    plot_confusion_matrix(cnf_matrix, classes=class_names,title='Confusion matrix')

    # Accuracy Calculation
    train_ac_score=accuracy_score(y_train,clf.predict(x_train))
    test_ac_score=accuracy_score(y_test,y_pred)
    print('Training Data Accuracy Score: %1.4f' % train_ac_score)
    print('Test Data Score: %1.4f' % test_ac_score)

    # Feature Importances
    feature_importances(x_train,clf)

    # Tree Visualization
    #visualize_tree(clf,class_names))

if __name__ == '__main__':
    main()
